{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "transformer model"
      ],
      "metadata": {
        "id": "0HrSPcxGa4xW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnBN7deF_K4X"
      },
      "outputs": [],
      "source": [
        "!pip  install transformers==4.22.1 -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers"
      ],
      "metadata": {
        "id": "iAEcIUa9_aW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(transformers.__version__)"
      ],
      "metadata": {
        "id": "RzqxKgUO_eCw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing stock ml libraries\n",
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "import logging\n",
        "logging.basicConfig(level=logging.ERROR)"
      ],
      "metadata": {
        "id": "OL0PKUAx_gqR"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Setting up the device for GPU usage\n",
        "\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "vfXc8FceBQAj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the performance of the Classification algorithm by calculating the percentage of its correct predictions.\n",
        "# takes in 4 params and create a empty list, which will store the accuracy score of the prediction\n",
        "def hamming_score(y_true, y_pred, normalize=True, sample_weight=None):\n",
        "    acc_list = []\n",
        "    for i in range(y_true.shape[0]):\n",
        "        set_true = set( np.where(y_true[i])[0] )\n",
        "        set_pred = set( np.where(y_pred[i])[0] )\n",
        "        tmp_a = None\n",
        "        if len(set_true) == 0 and len(set_pred) == 0:\n",
        "            tmp_a = 1\n",
        "        else:\n",
        "            tmp_a = len(set_true.intersection(set_pred))/\\\n",
        "                    float( len(set_true.union(set_pred)) )\n",
        "        acc_list.append(tmp_a)\n",
        "    return np.mean(acc_list)\n",
        "    # This code is used to calculate the accuracy of a prediction. \n",
        "    # It loops through each element in the true and predicted arrays, \n",
        "    # finds the intersection and union of the two sets, \n",
        "    # and then calculates the accuracy by taking the ratio of the intersection to the union. \n",
        "    # Finally, it returns the mean of all accuracy values."
      ],
      "metadata": {
        "id": "HDvmDdCtBV24"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing and Pre-Processing the domain data"
      ],
      "metadata": {
        "id": "JmhMPHgvT5iT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#This code is reading in a CSV file located at the given path and storing it in the variable 'data'. \n",
        "data = pd.read_csv('/content/Claim_train_data.csv')"
      ],
      "metadata": {
        "id": "CRR2tOaBAlAx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# showcase first 5 lines in the data set\n",
        "data.head()"
      ],
      "metadata": {
        "id": "90ELIp6eBdKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing the Dataset and Dataloader"
      ],
      "metadata": {
        "id": "hrt2hH3LT9K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sections of config\n",
        "\n",
        "# Defining some key variables that will be used later on in the training\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 1\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)\n",
        "# This code sets the maximum length of text to be 128 characters, \n",
        "# the batch size for training and validation to 4, \n",
        "# the number of epochs to 1, the learning rate to 1, \n",
        "# and creates a tokenizer using DistilBertTokenizer from the pretrained 'distilbert-base-uncased' model,\n",
        "#  with truncation enabled and lowercase enabled."
      ],
      "metadata": {
        "id": "ozz3sDrt_ibe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLabelDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.description = dataframe.description\n",
        "        self.targets = self.data.claimType\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.description)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        text = str(self.description[index])\n",
        "        text = \" \".join(text.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n",
        "        }\n",
        "# This code creates a MultiLabelDataset object which is a subclass of the Dataset class. \n",
        "# It initializes the tokenizer, dataframe, description, targets and max_len variables. \n",
        "# The len method returns the length of the description list and the getitem method takes an index as an argument,\n",
        "# and returns a dictionary containing ids, mask, token_type_ids and targets tensors"
      ],
      "metadata": {
        "id": "01nhvcW-C5hE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the dataset and dataloader for the neural network\n",
        "\n",
        "train_size = 0.8\n",
        "train_data=data.sample(frac=train_size,random_state=200)\n",
        "test_data=data\n",
        "\n",
        "\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(data.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
        "\n",
        "training_set = MultiLabelDataset(train_data, tokenizer, MAX_LEN)\n",
        "testing_set = MultiLabelDataset(test_data, tokenizer, MAX_LEN)\n",
        "# This code is setting up a training and testing dataset. \n",
        "# The train_size variable is set to 0.8, which means that 80% of the data will be used for training and 20% for testing. \n",
        "# The data is then split into two datasets, train_data and test_data, using the sample() method with a random_state of 200. \n",
        "# Finally, the MultiLabelDataset() function is used to create two datasets for training and testing from the train_data and test_data variables respectively."
      ],
      "metadata": {
        "id": "mxnI_EQcFnQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n",
        "# This code is setting up two DataLoader objects, one for training and one for testing. \n",
        "# The train_params and test_params dictionaries are used to set the batch size, \n",
        "# whether to shuffle the data, and the number of workers used to load the data. \n",
        "# The **train_params and **test_params syntax is used to unpack the dictionaries into keyword arguments."
      ],
      "metadata": {
        "id": "GqOQ-gEZG2tV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating the Neural Network for Fine Tuning"
      ],
      "metadata": {
        "id": "YmMq3N2zUBk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the customized model, by adding a drop out and a dense layer on top of distil bert to get the final output for the model. \n",
        "\n",
        "class DistilBERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DistilBERTClass, self).__init__()\n",
        "        self.l1 = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.pre_classifier = torch.nn.Linear(768, 768)\n",
        "        self.dropout = torch.nn.Dropout(0.1)\n",
        "        self.classifier = torch.nn.Linear(768, 6)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
        "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = output_1[0]\n",
        "        pooler = hidden_state[:, 0]\n",
        "        pooler = self.pre_classifier(pooler)\n",
        "        pooler = torch.nn.Tanh()(pooler)\n",
        "        pooler = self.dropout(pooler)\n",
        "        output = self.classifier(pooler)\n",
        "        return output\n",
        "\n",
        "model = DistilBERTClass()\n",
        "model.to(device)\n",
        "# This code creates a DistilBERTClass object which is a subclass of the torch.nn.Module class. \n",
        "# It initializes the DistilBERTModel from a pre-trained model, creates two linear layers, and a dropout layer. \n",
        "# The forward method takes in input_ids, attention_mask, and token_type_ids as parameters and returns an output after passing them through the layers of the model. \n",
        "# The model is then moved to the device specified by the user."
      ],
      "metadata": {
        "id": "x5y9M_SBHfFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Function and Optimizer"
      ],
      "metadata": {
        "id": "-O8IKSyxUJSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(outputs, targets):\n",
        "    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n",
        "# This code defines a function called \"loss_fn\" which takes two arguments, \"outputs\" and \"targets\". \n",
        "# The function then uses the torch.nn.BCEWithLogitsLoss() method to calculate the binary cross entropy loss between the outputs and targets."
      ],
      "metadata": {
        "id": "uWzfBNq4HtsE"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "#This code is setting up an Adam optimizer for a model with the given parameters and learning rate. \n",
        "# The Adam optimizer is a type of gradient descent algorithm that helps to optimize the model's parameters for better performance.\n"
      ],
      "metadata": {
        "id": "7DWxcKM7Hwzu"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine Tuning the Model"
      ],
      "metadata": {
        "id": "n_bXjtOzUN31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    model.train()\n",
        "    for _,data in tqdm(enumerate(training_loader, 0)):\n",
        "        ids = data['ids'].to(device, dtype = torch.long)\n",
        "        mask = data['mask'].to(device, dtype = torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "        targets = data['targets'].to(device, dtype = torch.float)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        if _%5000==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "# This code is defining a training function that takes an epoch as an argument. \n",
        "# It then iterates through the training loader, assigning the ids, mask, token_type_ids and targets to the device. \n",
        "# It then calculates the output of the model using those values and calculates a loss using a loss function. \n",
        "# If _ is divisible by 5000, it prints out the epoch and loss. \n",
        "# Finally, it backpropagates the loss and updates the optimizer."
      ],
      "metadata": {
        "id": "qwM1eVl_HzuD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    train(epoch)\n",
        "# This code is looping through a range of epochs and running a train function for each epoch."
      ],
      "metadata": {
        "id": "SAUXFxCEZbvD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validating the Model"
      ],
      "metadata": {
        "id": "DphvEyp_URyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(testing_loader):\n",
        "    model.eval()\n",
        "    fin_targets=[]\n",
        "    fin_outputs=[]\n",
        "    with torch.no_grad():\n",
        "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
        "            ids = data['ids'].to(device, dtype = torch.long)\n",
        "            mask = data['mask'].to(device, dtype = torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
        "            targets = data['targets'].to(device, dtype = torch.float)\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets\n",
        "# This code is a validation function that uses the torch library to evaluate a model. \n",
        "# It takes in a testing loader as an argument and then iterates through it, \n",
        "# converting the data into the appropriate format for the model. \n",
        "# It then calculates the outputs and targets, which are stored in two separate lists. \n",
        "# Finally, it returns these two lists so that they can be used for further analysis."
      ],
      "metadata": {
        "id": "9gls4D_CTl1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs, targets = validation(testing_loader)\n",
        "\n",
        "final_outputs = np.array(outputs) >=0.5\n",
        "# This code is taking the outputs from the validation function and converting them into an array. \n",
        "# The final_outputs line is then comparing each value in the array to 0.5 and returning a boolean value (true or false) for each element in the array."
      ],
      "metadata": {
        "id": "mfno-bx1Tnty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_hamming_loss = metrics.hamming_loss(targets, final_outputs)\n",
        "val_hamming_score = hamming_score(np.array(targets), np.array(final_outputs))\n",
        "\n",
        "print(f\"Hamming Score = {val_hamming_score}\")\n",
        "print(f\"Hamming Loss = {val_hamming_loss}\")\n",
        "# This code is calculating the Hamming score and Hamming loss of two arrays. \n",
        "# The Hamming score is a measure of similarity between two binary strings, \n",
        "# while the Hamming loss is the fraction of incorrect bits in the comparison. \n",
        "# The code first calculates the Hamming loss using the metrics.hamming_loss() function, \n",
        "# then calculates the Hamming score using the hamming_score() function, and finally prints out both values."
      ],
      "metadata": {
        "id": "QvsSbsVJTqEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saving the Trained Model for inference"
      ],
      "metadata": {
        "id": "nEzCxULEUVGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the files for inference\n",
        "\n",
        "output_model_file = ''\n",
        "output_vocab_file = ''\n",
        "\n",
        "torch.save(model, output_model_file)\n",
        "tokenizer.save_vocabulary(output_vocab_file)\n",
        "\n",
        "print('Saved')\n",
        "# This code is saving a model and its associated vocabulary to two separate files. \n",
        "# The first line sets the output model file name, and the second line sets the output vocabulary file name.\n",
        "#  The torch.save() function saves the model to the output model file, and tokenizer.save_vocabulary() saves the vocabulary to the output vocabulary file. \n",
        "# Finally, the print statement prints out 'Saved' to indicate that the save was successful."
      ],
      "metadata": {
        "id": "GBD-09yXTsmj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}